{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model = pipeline(\n",
    "    task=\"zero-shot-classification\",\n",
    "    model=\"typeform/distilbert-base-uncased-mnli\"\n",
    ")\n",
    "ner = pipeline(\n",
    "    task=\"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ommon patterns to identify intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label =[\"social conversation\",\"factual information lookup\"]\n",
    "chat_content = [\"hi\",\"hello\",\"thanks\",\"thank you\"]\n",
    "search_content = [\"who\", \"what\", \"when\", \"where\", \"how does\",\"tell me about\"]\n",
    "SOCIAL_ACT_PATTERNS = {\n",
    "  \"GREETING\": [\"hi\", \"hello\", \"hey\"],\n",
    "  \"GRATITUDE\": [\"thank\", \"thanks\"],\n",
    "  \"PERMISSION_REQUEST\": [\"can i ask\", \"may i ask\"],\n",
    "  \"WELLBEING_QUERY\": [\"how are you\", \"how do you feel\"],\n",
    "}\n",
    "SOCIAL_ACT_RESPONSES = {\n",
    "  \"GREETING\": [\n",
    "      \"Hey ðŸ™‚ What can I help you with?\",\n",
    "      \"Hi there! How can I assist?\"\n",
    "  ],\n",
    "  \"GRATITUDE\": [\n",
    "      \"You're welcome!\",\n",
    "      \"Glad I could help ðŸ™‚\"\n",
    "  ],\n",
    "  \"PERMISSION_REQUEST\": [\n",
    "      \"Of course. Go ahead.\",\n",
    "      \"Sure, what do you want to ask?\"\n",
    "  ],\n",
    "  \"WELLBEING_QUERY\": [\n",
    "      \"I'm doing well â€” how can I help you today?\",\n",
    "      \"I'm just here to help you ðŸ™‚\"\n",
    "  ]\n",
    "}\n",
    "INFO_VERBS = [\"explain\", \"tell\", \"describe\", \"define\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "def looks_like_info_request(text):\n",
    "    return any(v in text for v in INFO_VERBS)\n",
    "\n",
    "def route_intent(text):\n",
    "  result=model(text,label)\n",
    "\n",
    "  top_labels,top_scores=result[\"labels\"],result[\"scores\"]\n",
    "  if(any(x in text for x in chat_content)):\n",
    "    final_decision = \"social conversation\"\n",
    "  elif(any(x in text for x in search_content)):\n",
    "    final_decision = \"factual information lookup\"\n",
    "  else:\n",
    "    if(top_scores[0]<=.55):\n",
    "      final_decision = \"factual information lookup\"\n",
    "    else:\n",
    "      final_decision = top_labels[0]\n",
    "\n",
    "  return final_decision\n",
    "\n",
    "def handle_social_conversation(text):\n",
    "  for key,value in SOCIAL_ACT_PATTERNS.items():\n",
    "    if any(x in text for x in value):\n",
    "      return key\n",
    "  return None\n",
    "\n",
    "def handle_factual_information_lookup(text):\n",
    "  print(\"Let me look that up for you.\")\n",
    "  return \"(Search results will appear here)\"\n",
    "\n",
    "def extract_entities(text):\n",
    "  entities = ner(text)\n",
    "  values={}\n",
    "\n",
    "  for entity in entities:\n",
    "    label = entity[\"entity_group\"]\n",
    "    word = entity[\"word\"]\n",
    "\n",
    "    values.setdefault(label, []).append(word)\n",
    "  return values\n",
    "\n",
    "def debug_decision(raw_text, intent, entities, response=None):\n",
    "    return {\n",
    "        \"input\": raw_text,\n",
    "        \"intent\": intent,\n",
    "        \"response\": response,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "def clean_text(text):\n",
    "\n",
    "  cleaned_text = re.sub(r'\\[.*?\\]', '', text)\n",
    "  cleaned_text = cleaned_text.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
    "  cleaned_text = cleaned_text.strip()\n",
    "\n",
    "  return cleaned_text\n",
    "\n",
    "def chunk_document(text):\n",
    "\n",
    "  raw_chunk = re.split(r'\\n{2,}',text)\n",
    "  refined_chunk=[]\n",
    "\n",
    "  for index,chunk in enumerate(raw_chunk):\n",
    "    chunk = chunk.strip()\n",
    "\n",
    "    if len(chunk) < 50:\n",
    "      continue\n",
    "    refined_obj={\n",
    "      \"chunk_id\":f\"Document_overview_{index}\",\n",
    "      \"chunk\":chunk,\n",
    "      \"chunk_length\":len(chunk),\n",
    "      \"num_sentences\":len(re.findall(r'[.!?]+', chunk)),\n",
    "      \"position\":index,\n",
    "      \"entities\":extract_entities(chunk)\n",
    "    }\n",
    "    \n",
    "    refined_chunk.append(refined_obj)\n",
    "\n",
    "  return refined_chunk\n",
    "\n",
    "def normalize_query(query):\n",
    "  return query.lower().strip()\n",
    "\n",
    "def extract_keywords(query):\n",
    "  return {t for t in tokenize(query) if len(t) >= 3}\n",
    "\n",
    "def score_chunk(chunk_dict,chunk_text,query_keywords,query_entities):\n",
    "  chunk_text = chunk_text.lower()\n",
    "  chunk_tokens = tokenize(chunk_text)\n",
    "  keyword_score= sum(1 for k in query_keywords if k in chunk_tokens)\n",
    "  \n",
    "  chunk_entities = chunk_dict.get(\"entities\",{})\n",
    "  entity_score=0\n",
    "  \n",
    "  for ent_typ,ents in query_entities.items():\n",
    "    entity_score+= len(set(ents) & set(chunk_entities.get(ent_typ,[])))\n",
    "  \n",
    "  return keyword_score + (2*entity_score)\n",
    "\n",
    "def retrieve_best_chunk(chunks,query,top_k=1):\n",
    "  normalized_query = normalize_query(query)\n",
    "  query_keywords = extract_keywords(normalized_query)\n",
    "  query_entities = extract_entities(query)\n",
    "  \n",
    "  scored=[]\n",
    "  \n",
    "  for chunk in chunks:\n",
    "    score = score_chunk(chunk,chunk['chunk'],query_keywords,query_entities)\n",
    "    scored.append((score,chunk))\n",
    "  \n",
    "  scored.sort(key= lambda x:x[0],reverse=True)\n",
    "  \n",
    "  return [c for s,c in scored if s>0][:top_k]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "[{'chunk_id': 'Document_overview_1', 'chunk': 'Machine Learning is a subfield of AI that enables systems to learn patterns from data instead of being explicitly programmed. \\nIt is widely used in recommendation systems, fraud detection, and speech recognition.', 'chunk_length': 212, 'num_sentences': 2, 'position': 1, 'entities': {'MISC': ['Machine Learning', 'AI']}}]\n"
     ]
    }
   ],
   "source": [
    "# Small demo document for testing chunking + retrieval pipeline\n",
    "RAW_DOC = \"\"\"\n",
    "Artificial Intelligence is a field of computer science focused on building systems that can perform tasks requiring human-like intelligence. \n",
    "These tasks include reasoning, learning, perception, and language understanding.\n",
    "\n",
    "Machine Learning is a subfield of AI that enables systems to learn patterns from data instead of being explicitly programmed. \n",
    "It is widely used in recommendation systems, fraud detection, and speech recognition.\n",
    "\n",
    "Natural Language Processing allows machines to understand and generate human language. \n",
    "Applications include chatbots, search engines, and automated customer support.\n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = clean_text(RAW_DOC)\n",
    "chunks=chunk_document(cleaned_text)\n",
    "document = {\n",
    "    \"doc_id\": \"AI_ML_NLP_overview\",\n",
    "    \"source\": \"wikipedia\",\n",
    "    \"raw_text\": RAW_DOC,\n",
    "    \"clean_text\": cleaned_text,\n",
    "    \"chunks\": chunks,\n",
    "    \"No. of chunks\":len(chunks)\n",
    "\n",
    "    }\n",
    "query = \"Explain the concept of Machine Learning.\"\n",
    "best_chunk = retrieve_best_chunk(chunks,query)\n",
    "print(best_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "while True:\n",
    "    raw_text = input(\"what do you have in mind today? \")\n",
    "    normalized_text = raw_text.lower()\n",
    "    if looks_like_info_request(normalized_text):\n",
    "        intent = \"factual information lookup\"\n",
    "    else:\n",
    "        intent = route_intent(normalized_text)\n",
    "\n",
    "    print(intent)\n",
    "\n",
    "    if intent == \"social conversation\":\n",
    "        social_act = handle_social_conversation(normalized_text)\n",
    "        if social_act:\n",
    "            response = random.choice(SOCIAL_ACT_RESPONSES[social_act])\n",
    "        else:\n",
    "            response = random.choice(SOCIAL_ACT_RESPONSES[\"GREETING\"])\n",
    "    else:\n",
    "        response = handle_factual_information_lookup(raw_text)\n",
    "    entities = extract_entities(raw_text)\n",
    "    debug = debug_decision(raw_text, intent, entities, response)\n",
    "    print(debug)\n",
    "    print(\"press 1 to exit\")\n",
    "    if input() == \"1\":\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
