{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0d29b9987d4eb7825ab895dabde4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b73682a6cd94b6aab4f023a3b17afcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f97cfce0a7c4db9aeed1742829b7bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0501445bce8471c8783918f2e91ddf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722d003fe56e4880a0dfeea60b8f4efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d74679c8f942358a8aa66bff9dd10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5150d7a81fd64f5c887ee3c6f6d52535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model = pipeline(\n",
    "    task=\"zero-shot-classification\",\n",
    "    model=\"typeform/distilbert-base-uncased-mnli\"\n",
    ")\n",
    "ner = pipeline(\n",
    "    task=\"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "summarizer = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\"\n",
    ")\n",
    "generator = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model = \"gpt2-medium\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ommon patterns to identify intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label =[\"social conversation\",\"factual information lookup\"]\n",
    "chat_content = [\"hi\",\"hello\",\"thanks\",\"thank you\"]\n",
    "search_content = [\"who\", \"what\", \"when\", \"where\", \"how does\",\"tell me about\"]\n",
    "SOCIAL_ACT_PATTERNS = {\n",
    "  \"GREETING\": [\"hi\", \"hello\", \"hey\"],\n",
    "  \"GRATITUDE\": [\"thank\", \"thanks\"],\n",
    "  \"PERMISSION_REQUEST\": [\"can i ask\", \"may i ask\"],\n",
    "  \"WELLBEING_QUERY\": [\"how are you\", \"how do you feel\"],\n",
    "}\n",
    "SOCIAL_ACT_RESPONSES = {\n",
    "  \"GREETING\": [\n",
    "      \"Hey ðŸ™‚ What can I help you with?\",\n",
    "      \"Hi there! How can I assist?\"\n",
    "  ],\n",
    "  \"GRATITUDE\": [\n",
    "      \"You're welcome!\",\n",
    "      \"Glad I could help ðŸ™‚\"\n",
    "  ],\n",
    "  \"PERMISSION_REQUEST\": [\n",
    "      \"Of course. Go ahead.\",\n",
    "      \"Sure, what do you want to ask?\"\n",
    "  ],\n",
    "  \"WELLBEING_QUERY\": [\n",
    "      \"I'm doing well â€” how can I help you today?\",\n",
    "      \"I'm just here to help you ðŸ™‚\"\n",
    "  ]\n",
    "}\n",
    "INFO_VERBS = [\"explain\", \"tell\", \"describe\", \"define\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "def looks_like_info_request(text):\n",
    "    return any(v in text for v in INFO_VERBS)\n",
    "\n",
    "def route_intent(text):\n",
    "  result=model(text,label)\n",
    "\n",
    "  top_labels,top_scores=result[\"labels\"],result[\"scores\"]\n",
    "  if(any(x in text for x in chat_content)):\n",
    "    final_decision = \"social conversation\"\n",
    "  elif(any(x in text for x in search_content)):\n",
    "    final_decision = \"factual information lookup\"\n",
    "  else:\n",
    "    if(top_scores[0]<=.55):\n",
    "      final_decision = \"factual information lookup\"\n",
    "    else:\n",
    "      final_decision = top_labels[0]\n",
    "\n",
    "  return final_decision\n",
    "\n",
    "def handle_social_conversation(text):\n",
    "  for key,value in SOCIAL_ACT_PATTERNS.items():\n",
    "    if any(x in text for x in value):\n",
    "      return key\n",
    "  return None\n",
    "\n",
    "def handle_factual_information_lookup(text):\n",
    "  print(\"Let me look that up for you.\")\n",
    "  return \"(Search results will appear here)\"\n",
    "\n",
    "def extract_entities(text):\n",
    "  entities = ner(text)\n",
    "  values={}\n",
    "\n",
    "  for entity in entities:\n",
    "    label = entity[\"entity_group\"]\n",
    "    word = entity[\"word\"]\n",
    "\n",
    "    values.setdefault(label, []).append(word)\n",
    "  return values\n",
    "\n",
    "def debug_decision(raw_text, intent, entities, response=None):\n",
    "    return {\n",
    "        \"input\": raw_text,\n",
    "        \"intent\": intent,\n",
    "        \"response\": response,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "def clean_text(text):\n",
    "\n",
    "  cleaned_text = re.sub(r'\\[.*?\\]', '', text)\n",
    "  cleaned_text = re.sub(r'[\\r\\n]+', '\\n', cleaned_text)\n",
    "  cleaned_text = cleaned_text.strip()\n",
    "\n",
    "  return cleaned_text\n",
    "\n",
    "def chunk_document(text):\n",
    "\n",
    "  raw_chunk = re.split(r'\\n{2,}',text)\n",
    "  refined_chunk=[]\n",
    "\n",
    "  for index,chunk in enumerate(raw_chunk):\n",
    "    chunk = chunk.strip()\n",
    "\n",
    "    if len(chunk) < 50:\n",
    "      continue\n",
    "    refined_obj={\n",
    "      \"chunk_id\":f\"Document_overview_{index}\",\n",
    "      \"chunk\":chunk,\n",
    "      \"chunk_length\":len(chunk),\n",
    "      \"num_sentences\":len(re.findall(r'[.!?]+', chunk)),\n",
    "      \"position\":index,\n",
    "      \"entities\":extract_entities(chunk)\n",
    "    }\n",
    "    \n",
    "    refined_chunk.append(refined_obj)\n",
    "\n",
    "  return refined_chunk\n",
    "\n",
    "def normalize_query(query):\n",
    "  return query.lower().strip()\n",
    "\n",
    "def extract_keywords(query):\n",
    "  return {t for t in tokenize(query) if len(t) >= 3}\n",
    "\n",
    "def score_chunk(chunk_dict,chunk_text,query_keywords,query_entities):\n",
    "  chunk_text = chunk_text.lower()\n",
    "  chunk_tokens = tokenize(chunk_text)\n",
    "  keyword_score= sum(1 for k in query_keywords if k in chunk_tokens)\n",
    "  \n",
    "  chunk_entities = chunk_dict.get(\"entities\",{})\n",
    "  entity_score=0\n",
    "  \n",
    "  for ent_typ,ents in query_entities.items():\n",
    "    entity_score+= len(set(ents) & set(chunk_entities.get(ent_typ,[])))\n",
    "  \n",
    "  return keyword_score + (2*entity_score)\n",
    "\n",
    "def retrieve_best_chunk(chunks,query,top_k=1):\n",
    "  normalized_query = normalize_query(query)\n",
    "  query_keywords = extract_keywords(normalized_query)\n",
    "  query_entities = extract_entities(query)\n",
    "  \n",
    "  scored=[]\n",
    "  \n",
    "  for chunk in chunks:\n",
    "    score = score_chunk(chunk,chunk['chunk'],query_keywords,query_entities)\n",
    "    scored.append((score,chunk))\n",
    "  \n",
    "  scored.sort(key= lambda x:x[0],reverse=True)\n",
    "  \n",
    "  return [c for s,c in scored if s>0][:top_k]\n",
    "  \n",
    "def split_sentences(text):\n",
    "    return re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "def score_sentence(sentence, keywords):\n",
    "  sentence= sentence.lower().strip()\n",
    "  tokens = tokenize(sentence)\n",
    "  if not tokens:\n",
    "      return 0\n",
    "  return sum(1 for k in keywords if k in tokens)/len(tokens)\n",
    "  \n",
    "def summarize_chunk(chunk_text,max_sentence=2):\n",
    "  sentences = split_sentences(chunk_text)\n",
    "  if len(sentences)< 5:\n",
    "    keywords = extract_keywords(chunk_text)\n",
    "  else:\n",
    "    keywords = extract_keywords_filtered(chunk_text)\n",
    "  scored = []\n",
    "  for i,sentence in enumerate(sentences):\n",
    "    score = score_sentence(sentence,keywords)\n",
    "    if i==0:\n",
    "      score+=1.5\n",
    "    scored.append((score,sentence))\n",
    "  scored.sort(key=lambda x:x[0],reverse=True)\n",
    "  top_sentence = [s for _,s in scored[:max_sentence]]\n",
    "  top_sentence.sort(key=lambda x: sentences.index(x))\n",
    "  return \" \".join(top_sentence)\n",
    "from collections import Counter\n",
    "\n",
    "def extract_keywords_filtered(text, min_freq=2):\n",
    "    tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b',text.lower())\n",
    "    freq = Counter(tokens)\n",
    "    return {k for k, v in freq.items() if v >= min_freq}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are Jarvis, an intelligent, calm, and helpful AI assistant.\n",
      "You explain concepts clearly, avoid repetition, and speak like a thoughtful mentor.\n",
      "You give one short explanation and then stop.\n",
      "\n",
      "User: Explain Machine Learning in simple terms.\n",
      "Jarvis (2â€“3 sentences):\n",
      "User: Explain Machine Learning in simple terms.\n",
      "Jarvis:\n",
      "1) The algorithm is the core of all systems that work on data from any source; 2), It has been proven to be effective at solving problems when it can predict something useful about how you use your information or what kind will get selected as important for future training sets - but only after careful testing with real people/models! 3). If we don't understand our models correctly they fail miserably\n"
     ]
    }
   ],
   "source": [
    "# Action for text-generation\n",
    "jarvis_prompt = \"\"\"\n",
    "You are Jarvis, an intelligent, calm, and helpful AI assistant.\n",
    "You explain concepts clearly, avoid repetition, and speak like a thoughtful mentor.\n",
    "You give one short explanation and then stop.\n",
    "\n",
    "User: Explain Machine Learning in simple terms.\n",
    "Jarvis (2â€“3 sentences):\n",
    "User: Explain Machine Learning in simple terms.\n",
    "Jarvis:\n",
    "\"\"\"\n",
    "\n",
    "output = generator(\n",
    "    jarvis_prompt,\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    no_repeat_ngram_size=3,\n",
    "    eos_token_id=generator.tokenizer.eos_token_id\n",
    ")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action for Abstractive Summarization\n",
    "chunk_text = \"\"\"\n",
    "Machine Learning is a subfield of AI that enables systems to learn patterns\n",
    "from data instead of being explicitly programmed. It is widely used in\n",
    "recommendation systems, fraud detection, and speech recognition.\n",
    "\"\"\"\n",
    "extractive_summary = summarize_chunk(chunk_text)\n",
    "extractive_summary = re.sub(r'\\s+', ' ', extractive_summary).strip()\n",
    "\n",
    "summary = summarizer(\n",
    "    chunk_text,\n",
    "    max_length =30,\n",
    "    min_length=15,\n",
    "    do_sample=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Machine Learning is a subfield of AI that enables systems to learn patterns\n",
      "from data instead of being explicitly programmed. It is widely used in\n",
      "recommendation systems, fraud detection, and speech recognition.\n"
     ]
    }
   ],
   "source": [
    "# Action for Extractive Summarization\n",
    "\n",
    "chunk_text = \"\"\"\n",
    "Machine Learning is a subfield of AI that enables systems to learn patterns\n",
    "from data instead of being explicitly programmed. It is widely used in\n",
    "recommendation systems, fraud detection, and speech recognition.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_chunk(chunk_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action for Query\n",
    "\n",
    "# Small demo document for testing \n",
    "# chunking + retrieval pipeline\n",
    "RAW_DOC = \"\"\"\n",
    "Artificial Intelligence is a field of computer science focused on building systems that can perform tasks requiring human-like intelligence. \n",
    "These tasks include reasoning, learning, perception, and language understanding.\n",
    "\n",
    "Machine Learning is a subfield of AI that enables systems to learn patterns from data instead of being explicitly programmed. \n",
    "It is widely used in recommendation systems, fraud detection, and speech recognition.\n",
    "\n",
    "Natural Language Processing allows machines to understand and generate human language. \n",
    "Applications include chatbots, search engines, and automated customer support.\n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = clean_text(RAW_DOC)\n",
    "chunks=chunk_document(cleaned_text)\n",
    "document = {\n",
    "    \"doc_id\": \"AI_ML_NLP_overview\",\n",
    "    \"source\": \"wikipedia\",\n",
    "    \"raw_text\": RAW_DOC,\n",
    "    \"clean_text\": cleaned_text,\n",
    "    \"chunks\": chunks,\n",
    "    \"No. of chunks\":len(chunks)\n",
    "\n",
    "    }\n",
    "query = \"Explain the concept of Machine Learning.\"\n",
    "best_chunk = retrieve_best_chunk(chunks,query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action for Chatbot\n",
    "\n",
    "while True:\n",
    "    raw_text = input(\"what do you have in mind today? \")\n",
    "    normalized_text = raw_text.lower()\n",
    "    if looks_like_info_request(normalized_text):\n",
    "        intent = \"factual information lookup\"\n",
    "    else:\n",
    "        intent = route_intent(normalized_text)\n",
    "\n",
    "    print(intent)\n",
    "\n",
    "    if intent == \"social conversation\":\n",
    "        social_act = handle_social_conversation(normalized_text)\n",
    "        if social_act:\n",
    "            response = random.choice(SOCIAL_ACT_RESPONSES[social_act])\n",
    "        else:\n",
    "            response = random.choice(SOCIAL_ACT_RESPONSES[\"GREETING\"])\n",
    "    else:\n",
    "        response = handle_factual_information_lookup(raw_text)\n",
    "    entities = extract_entities(raw_text)\n",
    "    debug = debug_decision(raw_text, intent, entities, response)\n",
    "    print(debug)\n",
    "    print(\"press 1 to exit\")\n",
    "    if input() == \"1\":\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
